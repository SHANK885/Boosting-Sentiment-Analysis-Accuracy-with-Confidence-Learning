





import sagemaker
import logging
import boto3
import sagemaker
import pandas as pd
import json
import botocore
from botocore.exceptions import ClientError

config = botocore.config.Config(user_agent_extra='dlai-pds/c2/w3')

# low-level service client of the boto3 session
sm = boto3.client(service_name='sagemaker', 
                  config=config)

sm_runtime = boto3.client('sagemaker-runtime',
                          config=config)

sess = sagemaker.Session(sagemaker_client=sm,
                         sagemaker_runtime_client=sm_runtime)

bucket = sess.default_bucket()
role = sagemaker.get_execution_role()
region = sess.boto_region_name





from sagemaker.inputs import TrainingInput

# Set the path to the train data
train_data_path = 's3://sagemaker-us-east-1-670356074249/data/train.csv'

# TODO: set the path to the train data
train_data = TrainingInput(
    train_data_path, 
    content_type='application/x-sagemaker-training-data'
)






from sagemaker.pytorch import PyTorch

# TODO: create the estimator
estimator = PyTorch(
    entry_point= "main.py",
    source_dir= ...,
    base_job_name="sagemaker-script-mode",
    role=role,
    instance_count=1,
    instance_type="ml.p3.2xlarge",
    framework_version="2.1",
    py_version="py310",
    dependencies= ...,
    output_data_config={
        'S3OutputPath': ...
    },
    output_path= ...,
    environment={'PYTHONPATH': 'src'}
)





# Save the best model during training by specifying the output path
# (Note: The output path should be where the best model will be saved within the S3 bucket)
model_checkpoint = {
    'ModelCheckpoint': {
        'monitor': 'dev_loss',
        'dirpath': '/opt/ml/model/',
        'filename': 'best_model',
        'save_top_k': 1,
        'mode': 'min'
    }
}

# Attach the ModelCheckpoint callback to the estimator
estimator._hyperparameters['callbacks'] = [model_checkpoint]






# TODO: train the model
estimator.fit({...})






#TODO: copy the training artifacts from the S3 bucket to the current working directory
!aws s3 cp ...      





!tar -xzf output.tar.gz -C extracted_training_artifacts






# TODO: deploy the trained model
predictor = estimator.deploy(...)



